{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "63989eff0d164911a42e256e54955563",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Project 1: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "db1acc9836c2412e858abfb4a4bf775c",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Equipo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c33c8ce3f13e4570b50078c8bac26b4b",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- Jesús Valentín Niño Castañeda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4c2cbbea56a143a08d776b651bfaa724",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- Angel Toshio Tribeño Hurtado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7e37bf8e7b6540ad8b9f941813c4ea7b",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- Rafael Humberto Ramos Huamaní"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9bdccdbf69d346a1813484b3b119e000",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- Gabriel Vargas Urmeneta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "28386f12d85c401d8b1c8ca5d982872d",
    "deepnote_cell_type": "code",
    "execution_context_id": "86077cb8-b8bb-4eb6-aa7d-bac498369161",
    "execution_millis": 45952,
    "execution_start": 1759181563593,
    "source_hash": "b4731589"
   },
   "outputs": [],
   "source": [
    "%pip install h5py pyts tsfresh xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "1202498fe4094a1791cc61e4342ba1b3",
    "deepnote_cell_type": "code",
    "execution_context_id": "397ac21a-ebf7-47d6-9689-72ec3efc6e93",
    "execution_millis": 2017,
    "execution_start": 1759125776883,
    "source_hash": "29d82787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train keys: ['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z', 'y']\n",
      "Test keys: ['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"train.h5\", \"r\") as f:\n",
    "    print(\"Train keys:\", list(f.keys()))\n",
    "\n",
    "with h5py.File(\"test.h5\", \"r\") as f:\n",
    "    print(\"Test keys:\", list(f.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "f6094d1c7a1243bc8329c5afe86630c2",
    "deepnote_cell_type": "code",
    "execution_context_id": "397ac21a-ebf7-47d6-9689-72ec3efc6e93",
    "execution_millis": 780,
    "execution_start": 1759125779093,
    "source_hash": "97278cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7352, 128, 9)\n",
      "y_train shape: (7352,)\n",
      "X_test shape: (2947, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "SENSOR_KEYS = [\n",
    "    'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
    "    'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
    "    'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
    "]\n",
    "\n",
    "def load_h5_file(path, include_labels=True):\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        # Stack 9 sensor signals along last axis\n",
    "        signals = [np.array(f[k]) for k in SENSOR_KEYS]\n",
    "        X = np.stack(signals, axis=-1)   # shape (n_samples, n_timestamps, 9)\n",
    "        y = np.array(f['y']).flatten() if include_labels and 'y' in f else None\n",
    "    return X, y\n",
    "\n",
    "# Load train/test\n",
    "X_train, y_train = load_h5_file(\"train.h5\", include_labels=True)\n",
    "X_test, _ = load_h5_file(\"test.h5\", include_labels=False)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b3800cd24a62455d91ab0e40e67dc060",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## TsFresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "d55e3fe11df04fa2abf85676f8b67368",
    "deepnote_cell_type": "code",
    "execution_context_id": "c03445df-438b-4fa1-95cb-5fe105528f48",
    "execution_millis": 131863,
    "execution_start": 1759127200937,
    "source_hash": "e588ec68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 66168/66168 [00:24<00:00, 2697.92it/s]\n",
      "Feature Extraction: 100%|██████████| 26523/26523 [00:10<00:00, 2607.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsfresh features saved to CSV\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "\n",
    "SENSOR_KEYS = [\n",
    "    'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
    "    'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
    "    'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
    "]\n",
    "\n",
    "def load_h5_file(path, include_labels=True):\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        signals = [np.array(f[k]) for k in SENSOR_KEYS]\n",
    "        X = np.stack(signals, axis=-1)\n",
    "        y = np.array(f['y']).flatten() if include_labels and 'y' in f else None\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = load_h5_file(\"train.h5\", include_labels=True)\n",
    "X_test, _ = load_h5_file(\"test.h5\", include_labels=False)\n",
    "\n",
    "def to_long_dataframe(X):\n",
    "    n_samples, n_timestamps, n_features = X.shape\n",
    "    records = []\n",
    "    for sample in range(n_samples):\n",
    "        for feature in range(n_features):\n",
    "            for t in range(n_timestamps):\n",
    "                records.append({\n",
    "                    \"id\": sample,\n",
    "                    \"time\": t,\n",
    "                    \"kind\": f\"f{feature}\",\n",
    "                    \"value\": X[sample, t, feature]\n",
    "                })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df_train = to_long_dataframe(X_train)\n",
    "df_test  = to_long_dataframe(X_test)\n",
    "\n",
    "features_train = extract_features(\n",
    "    df_train,\n",
    "    column_id=\"id\",\n",
    "    column_sort=\"time\",\n",
    "    column_kind=\"kind\",\n",
    "    column_value=\"value\",\n",
    "    default_fc_parameters=MinimalFCParameters(),\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "features_test = extract_features(\n",
    "    df_test,\n",
    "    column_id=\"id\",\n",
    "    column_sort=\"time\",\n",
    "    column_kind=\"kind\",\n",
    "    column_value=\"value\",\n",
    "    default_fc_parameters=MinimalFCParameters(),\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "features_train[\"activity\"] = y_train\n",
    "features_train.to_csv(\"features_tsfresh_train.csv\", index=False)\n",
    "features_test.to_csv(\"features_tsfresh_test.csv\", index=False)\n",
    "\n",
    "print(\"tsfresh features saved to CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c10ff764b5084272840b039547b10232",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## PyTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "27c0bca9e9fa434086040526cd857669",
    "deepnote_cell_type": "code",
    "execution_context_id": "8c913bf7-c9a1-4d64-8fa9-f5fd28831d36",
    "execution_millis": 33050,
    "execution_start": 1759126504093,
    "source_hash": "f095f859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0:500/7352\n",
      "Processing 500:1000/7352\n",
      "Processing 1000:1500/7352\n",
      "Processing 1500:2000/7352\n",
      "Processing 2000:2500/7352\n",
      "Processing 2500:3000/7352\n",
      "Processing 3000:3500/7352\n",
      "Processing 3500:4000/7352\n",
      "Processing 4000:4500/7352\n",
      "Processing 4500:5000/7352\n",
      "Processing 5000:5500/7352\n",
      "Processing 5500:6000/7352\n",
      "Processing 6000:6500/7352\n",
      "Processing 6500:7000/7352\n",
      "Processing 7000:7352/7352\n",
      "Processing 0:500/2947\n",
      "Processing 500:1000/2947\n",
      "Processing 1000:1500/2947\n",
      "Processing 1500:2000/2947\n",
      "Processing 2000:2500/2947\n",
      "Processing 2500:2947/2947\n",
      "PyTS features extracted and saved\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pyts.image import RecurrencePlot\n",
    "\n",
    "SENSOR_KEYS = [\n",
    "    'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
    "    'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
    "    'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
    "]\n",
    "\n",
    "def load_h5_file(path, include_labels=True):\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        signals = [np.array(f[k]) for k in SENSOR_KEYS]\n",
    "        X = np.stack(signals, axis=-1)  # (n_samples, 128, 9)\n",
    "        y = np.array(f['y']).flatten() if include_labels and 'y' in f else None\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = load_h5_file(\"train.h5\", include_labels=True)\n",
    "X_test, _ = load_h5_file(\"test.h5\", include_labels=False)\n",
    "\n",
    "X_train_channel = X_train[:, :, 6]\n",
    "X_test_channel  = X_test[:, :, 6]\n",
    "\n",
    "def pyts_in_batches(X, batch_size=500, filename=\"output.npz\"):\n",
    "    rp = RecurrencePlot(threshold=\"point\", percentage=20)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    with open(filename, \"wb\") as f:\n",
    "        pass\n",
    "    \n",
    "    batches = []\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        print(f\"Processing {start}:{end}/{n_samples}\")\n",
    "        \n",
    "        batch = rp.fit_transform(X[start:end])\n",
    "        batch = batch.astype(np.float32)\n",
    "        batches.append(batch)\n",
    "    \n",
    "    X_rp = np.vstack(batches)\n",
    "    np.savez_compressed(filename, X_rp=X_rp)\n",
    "    return X_rp\n",
    "\n",
    "X_rp_train = pyts_in_batches(X_train_channel, batch_size=500, filename=\"features_pyts_train.npz\")\n",
    "np.savez_compressed(\"features_pyts_train.npz\", X_rp_train=X_rp_train, y_train=y_train)\n",
    "\n",
    "X_rp_test = pyts_in_batches(X_test_channel, batch_size=500, filename=\"features_pyts_test.npz\")\n",
    "np.savez_compressed(\"features_pyts_test.npz\", X_rp_test=X_rp_test)\n",
    "\n",
    "print(\"PyTS features extracted and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Testing\n",
    "\n",
    "In this section, we employ the `scikit-learn` and `xgboost` libraries to conduct an initial benchmarking of different classifiers.  \n",
    "The objective is twofold: (i) to identify the most suitable models to later implement from scratch, and (ii) to compare the performance of the feature extraction approaches (PyTS vs. TsFresh).  \n",
    "\n",
    "This preliminary evaluation provides guidance on which models and feature representations are most promising for the Human Activity Recognition task, ensuring that the manual implementations focus on methods with strong empirical performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "ffbf3585200f4076b90900c155b413b2",
    "deepnote_cell_type": "code",
    "execution_context_id": "86077cb8-b8bb-4eb6-aa7d-bac498369161",
    "execution_millis": 2653,
    "execution_start": 1759181628334,
    "source_hash": "adf5fefe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === TsFresh data ===\n",
    "features_train = pd.read_csv(\"features_tsfresh_train.csv\")\n",
    "features_test  = pd.read_csv(\"features_tsfresh_test.csv\")\n",
    "\n",
    "# Separate labels\n",
    "y_train = features_train[\"activity\"].astype(int) - 1\n",
    "X_train = features_train.drop(columns=[\"activity\"])\n",
    "X_test  = features_test\n",
    "\n",
    "# === PyTS data ===\n",
    "data_train = np.load(\"features_pyts_train.npz\")\n",
    "X_rp_train = data_train[\"X_rp_train\"]\n",
    "y_rp_train = data_train[\"y_train\"].astype(int) - 1\n",
    "\n",
    "data_test = np.load(\"features_pyts_test.npz\")\n",
    "X_rp_test = data_test[\"X_rp_test\"]\n",
    "\n",
    "# Flatten images for sklearn classifiers\n",
    "n_samples, h, w = X_rp_train.shape\n",
    "X_rp_train = X_rp_train.reshape(n_samples, -1)\n",
    "X_rp_test  = X_rp_test.reshape(X_rp_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "a1f26e19c32348caa780b964f84fa68e",
    "deepnote_cell_type": "code",
    "execution_context_id": "86077cb8-b8bb-4eb6-aa7d-bac498369161",
    "execution_millis": 78370,
    "execution_start": 1759181631044,
    "source_hash": "342596e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmarking Multiple Models with TsFresh data ===\n",
      "\n",
      ">>> Logistic Regression\n",
      "Accuracy: 0.938137321549966\n",
      "F1 Score: 0.9381845524468959\n",
      "Confusion Matrix:\n",
      " [[236  10   1   0   0   0]\n",
      " [  6 192   2   0   0   0]\n",
      " [ 10   4 192   0   0   0]\n",
      " [  0   0   0 234  28   0]\n",
      " [  0   0   0  30 246   0]\n",
      " [  0   0   0   0   0 280]]\n",
      "\n",
      ">>> Decision Tree\n",
      "Accuracy: 0.9401767505098573\n",
      "F1 Score: 0.9401592486401132\n",
      "Confusion Matrix:\n",
      " [[226  14   6   0   1   0]\n",
      " [ 20 173   7   0   0   0]\n",
      " [ 11   8 187   0   0   0]\n",
      " [  0   0   0 245  17   0]\n",
      " [  0   0   0   4 272   0]\n",
      " [  0   0   0   0   0 280]]\n",
      "\n",
      ">>> Random Forest\n",
      "Accuracy: 0.9789259007477906\n",
      "F1 Score: 0.9789219276974194\n",
      "Confusion Matrix:\n",
      " [[244   2   1   0   0   0]\n",
      " [  2 196   2   0   0   0]\n",
      " [  4   2 200   0   0   0]\n",
      " [  0   0   0 253   9   0]\n",
      " [  0   0   0   9 267   0]\n",
      " [  0   0   0   0   0 280]]\n",
      "\n",
      ">>> Naive Bayes\n",
      "Accuracy: 0.8103331067301156\n",
      "F1 Score: 0.8021684334964048\n",
      "Confusion Matrix:\n",
      " [[208  28  11   0   0   0]\n",
      " [ 37 151  12   0   0   0]\n",
      " [ 25   7 174   0   0   0]\n",
      " [  1   3   0 117 138   3]\n",
      " [  0   3   0  10 263   0]\n",
      " [  0   1   0   0   0 279]]\n",
      "\n",
      ">>> Linear SVM\n",
      "Accuracy: 0.9422161794697484\n",
      "F1 Score: 0.942269699301854\n",
      "Confusion Matrix:\n",
      " [[234  11   2   0   0   0]\n",
      " [  5 193   2   0   0   0]\n",
      " [  9   3 193   1   0   0]\n",
      " [  0   0   0 238  24   0]\n",
      " [  0   0   0  28 248   0]\n",
      " [  0   0   0   0   0 280]]\n",
      "\n",
      ">>> KNN (k=5)\n",
      "Accuracy: 0.9524133242692047\n",
      "F1 Score: 0.9524587063084201\n",
      "Confusion Matrix:\n",
      " [[243   4   0   0   0   0]\n",
      " [ 13 184   3   0   0   0]\n",
      " [ 11   4 191   0   0   0]\n",
      " [  0   0   0 246  16   0]\n",
      " [  0   0   0  18 258   0]\n",
      " [  0   0   0   1   0 279]]\n",
      "\n",
      ">>> XGBoost\n",
      "Accuracy: 0.9877634262406526\n",
      "F1 Score: 0.9877603444267434\n",
      "Confusion Matrix:\n",
      " [[246   1   0   0   0   0]\n",
      " [  2 198   0   0   0   0]\n",
      " [  3   2 201   0   0   0]\n",
      " [  0   0   0 254   8   0]\n",
      " [  0   0   0   2 274   0]\n",
      " [  0   0   0   0   0 280]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"=== Benchmarking Multiple Models with TsFresh data ===\")\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=2000),\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n>>> {name}\")\n",
    "    \n",
    "    if name in [\"Logistic Regression\", \"Naive Bayes\", \"Linear SVM\", \"RBF SVM\", \"KNN (k=5)\"]:\n",
    "        model.fit(X_tr_scaled, y_tr)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "    else:\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    results[name] = {\"Accuracy\": acc, \"F1\": f1}\n",
    "    \n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "allow_embed": false,
    "cell_id": "27d78c5595f24db5936ac6a264aed2f2",
    "deepnote_cell_type": "code",
    "execution_context_id": "86077cb8-b8bb-4eb6-aa7d-bac498369161",
    "execution_millis": 20479,
    "execution_start": 1759181709489,
    "source_hash": "cebc9337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmarking Multiple Models with PyTS data ===\n",
      "\n",
      ">>> Logistic Regression\n",
      "Accuracy: 0.504418762746431\n",
      "F1 Score: 0.5018065264523589\n",
      "Confusion Matrix:\n",
      " [[170  29  47   0   1   0]\n",
      " [ 37 119  41   0   0   3]\n",
      " [ 37  47 104  10   5   3]\n",
      " [  2   2   3  75 113  67]\n",
      " [  3   1   5  74 129  64]\n",
      " [  5   4   1  52  73 145]]\n",
      "\n",
      ">>> Decision Tree\n",
      "Accuracy: 0.2610469068660775\n",
      "F1 Score: 0.26108175277468004\n",
      "Confusion Matrix:\n",
      " [[84 46 42 22 29 24]\n",
      " [47 47 41 16 17 32]\n",
      " [42 45 44 30 20 25]\n",
      " [21 26 29 63 64 59]\n",
      " [32 30 22 66 70 56]\n",
      " [34 22 39 45 64 76]]\n",
      "\n",
      ">>> Random Forest\n",
      "Accuracy: 0.469068660774983\n",
      "F1 Score: 0.4614348757657078\n",
      "Confusion Matrix:\n",
      " [[175  28  21   5  13   5]\n",
      " [ 66 101   7   7   3  16]\n",
      " [ 67  32  48  13  28  18]\n",
      " [  2   0   0  81 117  62]\n",
      " [  0   2   1  88 150  35]\n",
      " [  1   0   1  54  89 135]]\n",
      "\n",
      ">>> Naive Bayes\n",
      "Accuracy: 0.42895989123045547\n",
      "F1 Score: 0.42940807607608267\n",
      "Confusion Matrix:\n",
      " [[115  60  64   2   6   0]\n",
      " [ 39 100  36   6  17   2]\n",
      " [ 43  39  72  12  40   0]\n",
      " [  0   0   0  65 162  35]\n",
      " [  0   2   1  78 169  26]\n",
      " [  0   0   2  58 110 110]]\n",
      "\n",
      ">>> Linear SVM\n",
      "Accuracy: 0.4004078857919782\n",
      "F1 Score: 0.39758757513189136\n",
      "Confusion Matrix:\n",
      " [[127  35  45  15  17   8]\n",
      " [ 33 107  30   5   6  19]\n",
      " [ 36  46  66  28  20  10]\n",
      " [ 15   8  16  68  96  59]\n",
      " [ 18  12  22  76  83  65]\n",
      " [ 15  20   7  39  61 138]]\n",
      "\n",
      ">>> KNN (k=5)\n",
      "Accuracy: 0.4969408565601632\n",
      "F1 Score: 0.41503865121996075\n",
      "Confusion Matrix:\n",
      " [[201  29  16   0   0   1]\n",
      " [ 29 148  16   0   0   7]\n",
      " [  6  35 161   0   0   4]\n",
      " [ 14  46  43  16   6 137]\n",
      " [ 30  41  38  18   5 144]\n",
      " [ 11  30  20  14   5 200]]\n",
      "\n",
      ">>> XGBoost\n",
      "Accuracy: 0.5112168592794017\n",
      "F1 Score: 0.5137037921061266\n",
      "Confusion Matrix:\n",
      " [[177  21  42   1   4   2]\n",
      " [ 32 120  37   1   1   9]\n",
      " [ 37  33 101  12  15   8]\n",
      " [  2   0   2  81 117  60]\n",
      " [  1   1   1 101 126  46]\n",
      " [  1   0   0  54  78 147]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"=== Benchmarking Multiple Models with PyTS data ===\")\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_rp_train, y_rp_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=2000),\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n>>> {name}\")\n",
    "    \n",
    "    if name in [\"Logistic Regression\", \"Naive Bayes\", \"Linear SVM\", \"RBF SVM\", \"KNN (k=5)\"]:\n",
    "        model.fit(X_tr_scaled, y_tr)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "    else:\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    results[name] = {\"Accuracy\": acc, \"F1\": f1}\n",
    "    \n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Conclusions\n",
    "\n",
    "The preliminary experiments revealed a clear difference between the two feature extraction approaches.  \n",
    "While the PyTS recurrence plot representation yielded poor classification performance (maximum weighted F1 ≈ 0.51 across models), the TsFresh feature extraction consistently led to strong results, with all models achieving weighted F1 scores above 0.80.  \n",
    "This confirms that TsFresh is substantially more effective at extracting informative features from the time-series data in the Human Activity Recognition dataset.\n",
    "\n",
    "Among the classifiers tested on TsFresh features, the best-performing models (ranked by weighted F1) were XGBoost, Random Forest, k-Nearest Neighbors (KNN), Linear SVM, Decision Tree, and Logistic Regression.  \n",
    "Although ensemble methods such as XGBoost and Random Forest achieved the highest scores, their complexity makes them less suitable for a from-scratch implementation in this project.  \n",
    "\n",
    "For this reason, we selected **Decision Tree** and **k-Nearest Neighbors** as the two models to implement manually. Both achieved competitive performance while being considerably simpler to code and interpret, and in addition, we already have prior experience implementing these algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def gini(y):\n",
    "    if len(y) == 0:\n",
    "        return 0.0\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probs = counts / counts.sum()\n",
    "    return 1.0 - np.sum(probs ** 2)\n",
    "\n",
    "def make_leaf(y, n_classes):\n",
    "    if len(y) == 0:\n",
    "        proba = np.zeros(n_classes)\n",
    "    else:\n",
    "        counts = np.bincount(y, minlength=n_classes)\n",
    "        proba = counts / counts.sum()\n",
    "    pred = np.argmax(proba)\n",
    "    return {\"leaf\": True, \"pred\": int(pred), \"proba\": proba,\n",
    "            \"feature\": None, \"thr\": None, \"cat\": None,\n",
    "            \"left\": None, \"right\": None}\n",
    "\n",
    "def make_cont_node(feature, thr, left, right):\n",
    "    return {\"leaf\": False, \"pred\": None, \"proba\": None,\n",
    "            \"feature\": feature, \"thr\": float(thr), \"cat\": None,\n",
    "            \"left\": left, \"right\": right}\n",
    "\n",
    "def make_cat_node(feature, cat, left, right):\n",
    "    return {\"leaf\": False, \"pred\": None, \"proba\": None,\n",
    "            \"feature\": feature, \"thr\": None, \"cat\": str(cat),\n",
    "            \"left\": left, \"right\": right}\n",
    "\n",
    "class DecisionTree:\n",
    "    def fit(self, X, y, cont_cols, cat_cols,\n",
    "            max_depth=10, min_samples_split=20, min_samples_leaf=10,\n",
    "            max_thr_candidates=64, random_state=42):\n",
    "\n",
    "        self.max_depth = int(max_depth)\n",
    "        self.min_samples_split = int(min_samples_split)\n",
    "        self.min_samples_leaf = int(min_samples_leaf)\n",
    "        self.max_thr_candidates = int(max_thr_candidates)\n",
    "        self.random_state = int(random_state)\n",
    "\n",
    "        self.cont_cols = list(cont_cols)\n",
    "        self.cat_cols = list(cat_cols)\n",
    "        self.cat_levels = {c: set(X[c].astype(str).unique()) for c in self.cat_cols}\n",
    "\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.root = self.grow(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        m = len(y)\n",
    "        best = {\"gain\": 0.0}\n",
    "        parent_imp = gini(y)\n",
    "        if m < self.min_samples_split or parent_imp == 0.0:\n",
    "            return None\n",
    "\n",
    "        for col in self.cont_cols:\n",
    "            values = X[col].astype(float).values\n",
    "            uniq = np.unique(values)\n",
    "            if len(uniq) <= 1:\n",
    "                continue\n",
    "            if len(uniq) > self.max_thr_candidates:\n",
    "                qs = np.linspace(0.05, 0.95, self.max_thr_candidates)\n",
    "                thr_cands = np.unique(np.quantile(values, qs))\n",
    "            else:\n",
    "                s = np.sort(uniq)\n",
    "                thr_cands = (s[:-1] + s[1:]) / 2.0\n",
    "\n",
    "            for thr in thr_cands:\n",
    "                left_idx = values <= thr\n",
    "                right_idx = ~left_idx\n",
    "                if left_idx.sum() < self.min_samples_leaf or right_idx.sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                g_left = gini(y[left_idx])\n",
    "                g_right = gini(y[right_idx])\n",
    "                gain = parent_imp - (left_idx.mean()*g_left + right_idx.mean()*g_right)\n",
    "                if gain > best[\"gain\"]:\n",
    "                    best = {\"gain\": gain, \"feature\": col, \"type\": \"cont\", \"thr\": float(thr),\n",
    "                            \"left_idx\": left_idx, \"right_idx\": right_idx}\n",
    "\n",
    "        for col in self.cat_cols:\n",
    "            vals = X[col].astype(str).values\n",
    "            for cat in np.unique(vals):\n",
    "                left_idx = (vals == cat)\n",
    "                right_idx = ~left_idx\n",
    "                if left_idx.sum() < self.min_samples_leaf or right_idx.sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                g_left = gini(y[left_idx])\n",
    "                g_right = gini(y[right_idx])\n",
    "                gain = parent_imp - (left_idx.mean()*g_left + right_idx.mean()*g_right)\n",
    "                if gain > best[\"gain\"]:\n",
    "                    best = {\"gain\": gain, \"feature\": col, \"type\": \"cat\", \"cat\": cat,\n",
    "                            \"left_idx\": left_idx, \"right_idx\": right_idx}\n",
    "\n",
    "        return best if best[\"gain\"] > 0.0 else None\n",
    "\n",
    "    def grow(self, X, y, depth):\n",
    "        if depth >= self.max_depth or len(y) < self.min_samples_split or gini(y) == 0.0:\n",
    "            return make_leaf(y, self.n_classes_)\n",
    "\n",
    "        split = self.best_split(X, y)\n",
    "        if split is None:\n",
    "            return make_leaf(y, self.n_classes_)\n",
    "\n",
    "        if split[\"type\"] == \"cont\":\n",
    "            left = self.grow(X[split[\"left_idx\"]], y[split[\"left_idx\"]], depth+1)\n",
    "            right = self.grow(X[split[\"right_idx\"]], y[split[\"right_idx\"]], depth+1)\n",
    "            return make_cont_node(split[\"feature\"], split[\"thr\"], left, right)\n",
    "        else:\n",
    "            left = self.grow(X[split[\"left_idx\"]], y[split[\"left_idx\"]], depth+1)\n",
    "            right = self.grow(X[split[\"right_idx\"]], y[split[\"right_idx\"]], depth+1)\n",
    "            return make_cat_node(split[\"feature\"], split[\"cat\"], left, right)\n",
    "\n",
    "    def predict_row(self, row, node):\n",
    "        while not node[\"leaf\"]:\n",
    "            val = row[node[\"feature\"]]\n",
    "            if node[\"thr\"] is not None:\n",
    "                go_left = float(val) <= node[\"thr\"]\n",
    "            else:\n",
    "                go_left = str(val) == node[\"cat\"]\n",
    "            node = node[\"left\"] if go_left else node[\"right\"]\n",
    "        return node[\"pred\"], node[\"proba\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        out = []\n",
    "        for _, r in X.iterrows():\n",
    "            p, _ = self.predict_row(r, self.root)\n",
    "            out.append(p)\n",
    "        return np.array(out, dtype=int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        out = []\n",
    "        for _, r in X.iterrows():\n",
    "            _, pr = self.predict_row(r, self.root)\n",
    "            out.append(pr)\n",
    "        return np.array(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Decision Tree Results:\n",
      "Accuracy: 0.9211420802175391\n",
      "F1 Score: 0.9210428000281494\n",
      "Confusion Matrix:\n",
      " [[210  21  14   0   0   0]\n",
      " [ 22 181  11   1   0   0]\n",
      " [ 11   9 177   0   0   0]\n",
      " [  0   0   0 241  16   0]\n",
      " [  0   0   0  11 264   0]\n",
      " [  0   0   0   0   0 282]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       245\n",
      "           1       0.86      0.84      0.85       215\n",
      "           2       0.88      0.90      0.89       197\n",
      "           3       0.95      0.94      0.95       257\n",
      "           4       0.94      0.96      0.95       275\n",
      "           5       1.00      1.00      1.00       282\n",
      "\n",
      "    accuracy                           0.92      1471\n",
      "   macro avg       0.92      0.92      0.92      1471\n",
      "weighted avg       0.92      0.92      0.92      1471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "tree = DecisionTree()\n",
    "tree.fit(\n",
    "    X_tr, y_tr,\n",
    "    cont_cols=X_tr.columns,\n",
    "    cat_cols=[],\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10\n",
    ")\n",
    "\n",
    "y_pred = tree.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "print(\"Custom Decision Tree Results:\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree()\n",
    "tree.fit(\n",
    "    X_train, y_train,\n",
    "    cont_cols=X_train.columns,\n",
    "    cat_cols=[]\n",
    ")\n",
    "y_pred_test = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: (2947,)\n",
      "First 10 test predictions: [5 5 5 5 5 5 5 5 4 5]\n",
      "submissionDT.csv saved\n"
     ]
    }
   ],
   "source": [
    "print(\"Test predictions shape:\", y_pred_test.shape)\n",
    "print(\"First 10 test predictions:\", y_pred_test[:10] + 1)\n",
    "y_pred_test = np.array(y_pred_test, dtype=int)\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": np.arange(1, len(y_pred_test) + 1),\n",
    "    \"Activity\": y_pred_test\n",
    "})\n",
    "assert submission.shape[0] == 2947, \"Row count must be 2947!\"\n",
    "submission.to_csv(\"submissionDT.csv\", index=False)\n",
    "print(\"submissionDT.csv saved\")"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "720790fa37304849b6c3457771488ff4",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
