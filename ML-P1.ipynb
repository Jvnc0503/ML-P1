{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "63989eff0d164911a42e256e54955563",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Project 1: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "db1acc9836c2412e858abfb4a4bf775c",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Team:\n",
    "- Jesús Valentín Niño Castañeda\n",
    "- Angel Toshio Tribeño Hurtado\n",
    "- Rafael Humberto Ramos Huamaní\n",
    "- Gabriel Vargas Urmeneta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "28386f12d85c401d8b1c8ca5d982872d",
    "deepnote_cell_type": "code",
    "execution_context_id": "86077cb8-b8bb-4eb6-aa7d-bac498369161",
    "execution_millis": 45952,
    "execution_start": 1759181563593,
    "source_hash": "b4731589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (3.14.0)\n",
      "Requirement already satisfied: pyts in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: tsfresh in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from h5py) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.1 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from pyts) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from pyts) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from pyts) (1.5.2)\n",
      "Requirement already satisfied: numba>=0.55.2 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from pyts) (0.62.1)\n",
      "Requirement already satisfied: requests>=2.9.1 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from tsfresh) (2.32.5)\n",
      "Requirement already satisfied: pandas>=0.25.0 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from tsfresh) (2.2.3)\n",
      "Requirement already satisfied: statsmodels>=0.13 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from tsfresh) (0.14.5)\n",
      "Requirement already satisfied: patsy>=0.4.1 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from tsfresh) (1.0.1)\n",
      "Requirement already satisfied: pywavelets in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from tsfresh) (1.8.0)\n",
      "Requirement already satisfied: tqdm>=4.10.0 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from tsfresh) (4.67.1)\n",
      "Requirement already satisfied: stumpy>=1.7.2 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from tsfresh) (1.13.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from tsfresh) (3.1.1)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from numba>=0.55.2->pyts) (0.45.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from pandas>=0.25.0->tsfresh) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from pandas>=0.25.0->tsfresh) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from pandas>=0.25.0->tsfresh) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25.0->tsfresh) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from requests>=2.9.1->tsfresh) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from requests>=2.9.1->tsfresh) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from requests>=2.9.1->tsfresh) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from requests>=2.9.1->tsfresh) (2025.8.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from scikit-learn>=1.2.0->pyts) (3.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from statsmodels>=0.13->tsfresh) (25.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from tqdm>=4.10.0->tsfresh) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install h5py pyts tsfresh xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "1202498fe4094a1791cc61e4342ba1b3",
    "deepnote_cell_type": "code",
    "execution_context_id": "397ac21a-ebf7-47d6-9689-72ec3efc6e93",
    "execution_millis": 2017,
    "execution_start": 1759125776883,
    "source_hash": "29d82787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train keys: ['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z', 'y']\n",
      "Test keys: ['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"train.h5\", \"r\") as f:\n",
    "    print(\"Train keys:\", list(f.keys()))\n",
    "\n",
    "with h5py.File(\"test.h5\", \"r\") as f:\n",
    "    print(\"Test keys:\", list(f.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "f6094d1c7a1243bc8329c5afe86630c2",
    "deepnote_cell_type": "code",
    "execution_context_id": "397ac21a-ebf7-47d6-9689-72ec3efc6e93",
    "execution_millis": 780,
    "execution_start": 1759125779093,
    "source_hash": "97278cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7352, 128, 9)\n",
      "y_train shape: (7352,)\n",
      "X_test shape: (2947, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "SENSOR_KEYS = [\n",
    "    'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
    "    'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
    "    'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
    "]\n",
    "\n",
    "def load_h5_file(path, include_labels=True):\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        # Stack 9 sensor signals along last axis\n",
    "        signals = [np.array(f[k]) for k in SENSOR_KEYS]\n",
    "        X = np.stack(signals, axis=-1)   # shape (n_samples, n_timestamps, 9)\n",
    "        y = np.array(f['y']).flatten() if include_labels and 'y' in f else None\n",
    "    return X, y\n",
    "\n",
    "# Load train/test\n",
    "X_train, y_train = load_h5_file(\"train.h5\", include_labels=True)\n",
    "X_test, _ = load_h5_file(\"test.h5\", include_labels=False)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b3800cd24a62455d91ab0e40e67dc060",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## TsFresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "d55e3fe11df04fa2abf85676f8b67368",
    "deepnote_cell_type": "code",
    "execution_context_id": "c03445df-438b-4fa1-95cb-5fe105528f48",
    "execution_millis": 131863,
    "execution_start": 1759127200937,
    "source_hash": "e588ec68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 66168/66168 [00:26<00:00, 2500.43it/s]\n",
      "Feature Extraction: 100%|██████████| 26523/26523 [00:11<00:00, 2222.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsfresh features saved to CSV\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "\n",
    "SENSOR_KEYS = [\n",
    "    'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
    "    'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
    "    'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
    "]\n",
    "\n",
    "def load_h5_file(path, include_labels=True):\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        signals = [np.array(f[k]) for k in SENSOR_KEYS]\n",
    "        X = np.stack(signals, axis=-1)\n",
    "        y = np.array(f['y']).flatten() if include_labels and 'y' in f else None\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = load_h5_file(\"train.h5\", include_labels=True)\n",
    "X_test, _ = load_h5_file(\"test.h5\", include_labels=False)\n",
    "\n",
    "def to_long_dataframe(X):\n",
    "    n_samples, n_timestamps, n_features = X.shape\n",
    "    records = []\n",
    "    for sample in range(n_samples):\n",
    "        for feature in range(n_features):\n",
    "            for t in range(n_timestamps):\n",
    "                records.append({\n",
    "                    \"id\": sample,\n",
    "                    \"time\": t,\n",
    "                    \"kind\": f\"f{feature}\",\n",
    "                    \"value\": X[sample, t, feature]\n",
    "                })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df_train = to_long_dataframe(X_train)\n",
    "df_test  = to_long_dataframe(X_test)\n",
    "\n",
    "features_train = extract_features(\n",
    "    df_train,\n",
    "    column_id=\"id\",\n",
    "    column_sort=\"time\",\n",
    "    column_kind=\"kind\",\n",
    "    column_value=\"value\",\n",
    "    default_fc_parameters=MinimalFCParameters(),\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "features_test = extract_features(\n",
    "    df_test,\n",
    "    column_id=\"id\",\n",
    "    column_sort=\"time\",\n",
    "    column_kind=\"kind\",\n",
    "    column_value=\"value\",\n",
    "    default_fc_parameters=MinimalFCParameters(),\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "features_train[\"activity\"] = y_train\n",
    "features_train.to_csv(\"features_tsfresh_train.csv\", index=False)\n",
    "features_test.to_csv(\"features_tsfresh_test.csv\", index=False)\n",
    "\n",
    "print(\"tsfresh features saved to CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c10ff764b5084272840b039547b10232",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## PyTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "27c0bca9e9fa434086040526cd857669",
    "deepnote_cell_type": "code",
    "execution_context_id": "8c913bf7-c9a1-4d64-8fa9-f5fd28831d36",
    "execution_millis": 33050,
    "execution_start": 1759126504093,
    "source_hash": "f095f859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0:500/7352\n",
      "Processing 500:1000/7352\n",
      "Processing 1000:1500/7352\n",
      "Processing 1500:2000/7352\n",
      "Processing 2000:2500/7352\n",
      "Processing 2500:3000/7352\n",
      "Processing 3000:3500/7352\n",
      "Processing 3500:4000/7352\n",
      "Processing 4000:4500/7352\n",
      "Processing 4500:5000/7352\n",
      "Processing 5000:5500/7352\n",
      "Processing 5500:6000/7352\n",
      "Processing 6000:6500/7352\n",
      "Processing 6500:7000/7352\n",
      "Processing 7000:7352/7352\n",
      "Processing 0:500/2947\n",
      "Processing 500:1000/2947\n",
      "Processing 1000:1500/2947\n",
      "Processing 1500:2000/2947\n",
      "Processing 2000:2500/2947\n",
      "Processing 2500:2947/2947\n",
      "PyTS features extracted and saved\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pyts.image import RecurrencePlot\n",
    "\n",
    "SENSOR_KEYS = [\n",
    "    'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
    "    'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
    "    'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
    "]\n",
    "\n",
    "def load_h5_file(path, include_labels=True):\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        signals = [np.array(f[k]) for k in SENSOR_KEYS]\n",
    "        X = np.stack(signals, axis=-1)  # (n_samples, 128, 9)\n",
    "        y = np.array(f['y']).flatten() if include_labels and 'y' in f else None\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = load_h5_file(\"train.h5\", include_labels=True)\n",
    "X_test, _ = load_h5_file(\"test.h5\", include_labels=False)\n",
    "\n",
    "X_train_channel = X_train[:, :, 6]\n",
    "X_test_channel  = X_test[:, :, 6]\n",
    "\n",
    "def pyts_in_batches(X, batch_size=500, filename=\"output.npz\"):\n",
    "    rp = RecurrencePlot(threshold=\"point\", percentage=20)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    with open(filename, \"wb\") as f:\n",
    "        pass\n",
    "    \n",
    "    batches = []\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        print(f\"Processing {start}:{end}/{n_samples}\")\n",
    "        \n",
    "        batch = rp.fit_transform(X[start:end])\n",
    "        batch = batch.astype(np.float32)\n",
    "        batches.append(batch)\n",
    "    \n",
    "    X_rp = np.vstack(batches)\n",
    "    np.savez_compressed(filename, X_rp=X_rp)\n",
    "    return X_rp\n",
    "\n",
    "X_rp_train = pyts_in_batches(X_train_channel, batch_size=500, filename=\"features_pyts_train.npz\")\n",
    "np.savez_compressed(\"features_pyts_train.npz\", X_rp_train=X_rp_train, y_train=y_train)\n",
    "\n",
    "X_rp_test = pyts_in_batches(X_test_channel, batch_size=500, filename=\"features_pyts_test.npz\")\n",
    "np.savez_compressed(\"features_pyts_test.npz\", X_rp_test=X_rp_test)\n",
    "\n",
    "print(\"PyTS features extracted and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Testing\n",
    "\n",
    "En esta sección, empleamos las bibliotecas `scikit-learn` y `xgboost` para realizar una evaluación comparativa inicial de diferentes clasificadores.\n",
    "\n",
    "El objetivo es doble: (i) identificar los modelos más adecuados para luego implementar desde cero, y (ii) comparar el rendimiento de los enfoques de extracción de características (PyTS vs. TsFresh).\n",
    "\n",
    "Esta evaluación preliminar proporciona orientación sobre qué modelos y representaciones de características son más prometedores para la tarea de Reconocimiento de Actividad Humana, asegurando que las implementaciones manuales se centren en métodos con un sólido rendimiento empírico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "ffbf3585200f4076b90900c155b413b2",
    "deepnote_cell_type": "code",
    "execution_context_id": "86077cb8-b8bb-4eb6-aa7d-bac498369161",
    "execution_millis": 2653,
    "execution_start": 1759181628334,
    "source_hash": "adf5fefe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === TsFresh data ===\n",
    "features_train = pd.read_csv(\"features_tsfresh_train.csv\")\n",
    "features_test  = pd.read_csv(\"features_tsfresh_test.csv\")\n",
    "\n",
    "# separar labels\n",
    "y_train = features_train[\"activity\"].astype(int) - 1\n",
    "X_train = features_train.drop(columns=[\"activity\"])\n",
    "X_test  = features_test\n",
    "\n",
    "# === PyTS data ===\n",
    "data_train = np.load(\"features_pyts_train.npz\")\n",
    "X_rp_train = data_train[\"X_rp_train\"]\n",
    "y_rp_train = data_train[\"y_train\"].astype(int) - 1\n",
    "\n",
    "data_test = np.load(\"features_pyts_test.npz\")\n",
    "X_rp_test = data_test[\"X_rp_test\"]\n",
    "\n",
    "# Aplanar las imágenes para los clasificadores de sklearn.\n",
    "\n",
    "n_samples, h, w = X_rp_train.shape\n",
    "X_rp_train = X_rp_train.reshape(n_samples, -1)\n",
    "X_rp_test  = X_rp_test.reshape(X_rp_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "a1f26e19c32348caa780b964f84fa68e",
    "deepnote_cell_type": "code",
    "execution_context_id": "86077cb8-b8bb-4eb6-aa7d-bac498369161",
    "execution_millis": 78370,
    "execution_start": 1759181631044,
    "source_hash": "342596e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmarking Multiple Models with TsFresh data ===\n",
      "\n",
      ">>> Logistic Regression\n",
      "Accuracy: 0.938137321549966\n",
      "F1 Score: 0.9381939562274736\n",
      "Confusion Matrix:\n",
      " [[236  10   1   0   0   0]\n",
      " [  7 191   2   0   0   0]\n",
      " [ 10   4 192   0   0   0]\n",
      " [  0   0   0 235  27   0]\n",
      " [  0   0   0  30 246   0]\n",
      " [  0   0   0   0   0 280]]\n",
      "\n",
      ">>> Decision Tree\n",
      "Accuracy: 0.9401767505098573\n",
      "F1 Score: 0.9401592486401132\n",
      "Confusion Matrix:\n",
      " [[226  14   6   0   1   0]\n",
      " [ 20 173   7   0   0   0]\n",
      " [ 11   8 187   0   0   0]\n",
      " [  0   0   0 245  17   0]\n",
      " [  0   0   0   4 272   0]\n",
      " [  0   0   0   0   0 280]]\n",
      "\n",
      ">>> Random Forest\n",
      "Accuracy: 0.9789259007477906\n",
      "F1 Score: 0.9789219276974194\n",
      "Confusion Matrix:\n",
      " [[244   2   1   0   0   0]\n",
      " [  2 196   2   0   0   0]\n",
      " [  4   2 200   0   0   0]\n",
      " [  0   0   0 253   9   0]\n",
      " [  0   0   0   9 267   0]\n",
      " [  0   0   0   0   0 280]]\n",
      "\n",
      ">>> Naive Bayes\n",
      "Accuracy: 0.8103331067301156\n",
      "F1 Score: 0.8021684334964048\n",
      "Confusion Matrix:\n",
      " [[208  28  11   0   0   0]\n",
      " [ 37 151  12   0   0   0]\n",
      " [ 25   7 174   0   0   0]\n",
      " [  1   3   0 117 138   3]\n",
      " [  0   3   0  10 263   0]\n",
      " [  0   1   0   0   0 279]]\n",
      "\n",
      ">>> Linear SVM\n",
      "Accuracy: 0.9422161794697484\n",
      "F1 Score: 0.942269699301854\n",
      "Confusion Matrix:\n",
      " [[234  11   2   0   0   0]\n",
      " [  5 193   2   0   0   0]\n",
      " [  9   3 193   1   0   0]\n",
      " [  0   0   0 238  24   0]\n",
      " [  0   0   0  28 248   0]\n",
      " [  0   0   0   0   0 280]]\n",
      "\n",
      ">>> KNN (k=5)\n",
      "Accuracy: 0.9524133242692047\n",
      "F1 Score: 0.9524587063084201\n",
      "Confusion Matrix:\n",
      " [[243   4   0   0   0   0]\n",
      " [ 13 184   3   0   0   0]\n",
      " [ 11   4 191   0   0   0]\n",
      " [  0   0   0 246  16   0]\n",
      " [  0   0   0  18 258   0]\n",
      " [  0   0   0   1   0 279]]\n",
      "\n",
      ">>> XGBoost\n",
      "Accuracy: 0.9877634262406526\n",
      "F1 Score: 0.9877603444267434\n",
      "Confusion Matrix:\n",
      " [[246   1   0   0   0   0]\n",
      " [  2 198   0   0   0   0]\n",
      " [  3   2 201   0   0   0]\n",
      " [  0   0   0 254   8   0]\n",
      " [  0   0   0   2 274   0]\n",
      " [  0   0   0   0   0 280]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"=== Benchmarking Multiple Models with TsFresh data ===\")\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=2000),\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n>>> {name}\")\n",
    "    \n",
    "    if name in [\"Logistic Regression\", \"Naive Bayes\", \"Linear SVM\", \"RBF SVM\", \"KNN (k=5)\"]:\n",
    "        model.fit(X_tr_scaled, y_tr)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "    else:\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    results[name] = {\"Accuracy\": acc, \"F1\": f1}\n",
    "    \n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "allow_embed": false,
    "cell_id": "27d78c5595f24db5936ac6a264aed2f2",
    "deepnote_cell_type": "code",
    "execution_context_id": "86077cb8-b8bb-4eb6-aa7d-bac498369161",
    "execution_millis": 20479,
    "execution_start": 1759181709489,
    "source_hash": "cebc9337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmarking Multiple Models with PyTS data ===\n",
      "\n",
      ">>> Logistic Regression\n",
      "Accuracy: 0.504418762746431\n",
      "F1 Score: 0.5018065264523589\n",
      "Confusion Matrix:\n",
      " [[170  29  47   0   1   0]\n",
      " [ 37 119  41   0   0   3]\n",
      " [ 37  47 104  10   5   3]\n",
      " [  2   2   3  75 113  67]\n",
      " [  3   1   5  74 129  64]\n",
      " [  5   4   1  52  73 145]]\n",
      "\n",
      ">>> Decision Tree\n",
      "Accuracy: 0.2610469068660775\n",
      "F1 Score: 0.26108175277468004\n",
      "Confusion Matrix:\n",
      " [[84 46 42 22 29 24]\n",
      " [47 47 41 16 17 32]\n",
      " [42 45 44 30 20 25]\n",
      " [21 26 29 63 64 59]\n",
      " [32 30 22 66 70 56]\n",
      " [34 22 39 45 64 76]]\n",
      "\n",
      ">>> Random Forest\n",
      "Accuracy: 0.469068660774983\n",
      "F1 Score: 0.4614348757657078\n",
      "Confusion Matrix:\n",
      " [[175  28  21   5  13   5]\n",
      " [ 66 101   7   7   3  16]\n",
      " [ 67  32  48  13  28  18]\n",
      " [  2   0   0  81 117  62]\n",
      " [  0   2   1  88 150  35]\n",
      " [  1   0   1  54  89 135]]\n",
      "\n",
      ">>> Naive Bayes\n",
      "Accuracy: 0.42895989123045547\n",
      "F1 Score: 0.42940807607608267\n",
      "Confusion Matrix:\n",
      " [[115  60  64   2   6   0]\n",
      " [ 39 100  36   6  17   2]\n",
      " [ 43  39  72  12  40   0]\n",
      " [  0   0   0  65 162  35]\n",
      " [  0   2   1  78 169  26]\n",
      " [  0   0   2  58 110 110]]\n",
      "\n",
      ">>> Linear SVM\n",
      "Accuracy: 0.4004078857919782\n",
      "F1 Score: 0.39758757513189136\n",
      "Confusion Matrix:\n",
      " [[127  35  45  15  17   8]\n",
      " [ 33 107  30   5   6  19]\n",
      " [ 36  46  66  28  20  10]\n",
      " [ 15   8  16  68  96  59]\n",
      " [ 18  12  22  76  83  65]\n",
      " [ 15  20   7  39  61 138]]\n",
      "\n",
      ">>> KNN (k=5)\n",
      "Accuracy: 0.4969408565601632\n",
      "F1 Score: 0.41503865121996075\n",
      "Confusion Matrix:\n",
      " [[201  29  16   0   0   1]\n",
      " [ 29 148  16   0   0   7]\n",
      " [  6  35 161   0   0   4]\n",
      " [ 14  46  43  16   6 137]\n",
      " [ 30  41  38  18   5 144]\n",
      " [ 11  30  20  14   5 200]]\n",
      "\n",
      ">>> XGBoost\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_scaled)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m     43\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_val, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages\\xgboost\\sklearn.py:1683\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1661\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1662\u001b[0m     xgb_model, params, feature_weights\n\u001b[0;32m   1663\u001b[0m )\n\u001b[0;32m   1664\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1665\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1666\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1680\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1681\u001b[0m )\n\u001b[1;32m-> 1683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages\\xgboost\\training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages\\xgboost\\core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2246\u001b[0m     _check_call(\n\u001b[1;32m-> 2247\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2250\u001b[0m     )\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"=== Benchmarking Multiple Models with PyTS data ===\")\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_rp_train, y_rp_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=2000),\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n>>> {name}\")\n",
    "    \n",
    "    if name in [\"Logistic Regression\", \"Naive Bayes\", \"Linear SVM\", \"RBF SVM\", \"KNN (k=5)\"]:\n",
    "        model.fit(X_tr_scaled, y_tr)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "    else:\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    results[name] = {\"Accuracy\": acc, \"F1\": f1}\n",
    "    \n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis y Conclusiones\n",
    "\n",
    "Los experimentos preliminares revelaron una clara diferencia entre los dos enfoques de extracción de características.\n",
    "Mientras que la representación de gráficos de recurrencia de PyTS produjo un rendimiento de clasificación deficiente (F1 ponderado máximo ≈ 0.51 entre modelos), la extracción de características con TsFresh condujo consistentemente a resultados sólidos, con todos los modelos alcanzando puntuaciones F1 ponderadas superiores a 0.80.\n",
    "Esto confirma que TsFresh es sustancialmente más efectivo para extraer características informativas de los datos de series temporales en el conjunto de datos de Reconocimiento de Actividad Humana.\n",
    "\n",
    "Entre los clasificadores probados con las características de TsFresh, los modelos con mejor rendimiento (clasificados por F1 ponderado) fueron XGBoost, Random Forest, k-Vecinos Más Cercanos (KNN), SVM Lineal, Árbol de Decisión y Regresión Logística.\n",
    "Aunque los métodos de ensemble como XGBoost y Random Forest alcanzaron las puntuaciones más altas, su complejidad los hace menos adecuados para una implementación desde cero en este proyecto.\n",
    "\n",
    "Por esta razón, seleccionamos **Árbol de Decisión** y **k-Vecinos Más Cercanos** como los dos modelos para implementar manualmente. Ambos lograron un rendimiento competitivo mientras son considerablemente más simples de programar e interpretar, y además, ya tenemos experiencia previa implementando estos algoritmos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def gini(y):\n",
    "    if len(y) == 0:\n",
    "        return 0.0\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probs = counts / counts.sum()\n",
    "    return 1.0 - np.sum(probs ** 2)\n",
    "\n",
    "def make_leaf(y, n_classes):\n",
    "    if len(y) == 0:\n",
    "        proba = np.zeros(n_classes)\n",
    "    else:\n",
    "        counts = np.bincount(y, minlength=n_classes)\n",
    "        proba = counts / counts.sum()\n",
    "    pred = np.argmax(proba)\n",
    "    return {\"leaf\": True, \"pred\": int(pred), \"proba\": proba,\n",
    "            \"feature\": None, \"thr\": None, \"cat\": None,\n",
    "            \"left\": None, \"right\": None}\n",
    "\n",
    "def make_cont_node(feature, thr, left, right):\n",
    "    return {\"leaf\": False, \"pred\": None, \"proba\": None,\n",
    "            \"feature\": feature, \"thr\": float(thr), \"cat\": None,\n",
    "            \"left\": left, \"right\": right}\n",
    "\n",
    "def make_cat_node(feature, cat, left, right):\n",
    "    return {\"leaf\": False, \"pred\": None, \"proba\": None,\n",
    "            \"feature\": feature, \"thr\": None, \"cat\": str(cat),\n",
    "            \"left\": left, \"right\": right}\n",
    "\n",
    "class DecisionTree:\n",
    "    def fit(self, X, y, cont_cols, cat_cols,\n",
    "            max_depth=10, min_samples_split=20, min_samples_leaf=10,\n",
    "            max_thr_candidates=64, random_state=42):\n",
    "\n",
    "        self.max_depth = int(max_depth)\n",
    "        self.min_samples_split = int(min_samples_split)\n",
    "        self.min_samples_leaf = int(min_samples_leaf)\n",
    "        self.max_thr_candidates = int(max_thr_candidates)\n",
    "        self.random_state = int(random_state)\n",
    "\n",
    "        self.cont_cols = list(cont_cols)\n",
    "        self.cat_cols = list(cat_cols)\n",
    "        self.cat_levels = {c: set(X[c].astype(str).unique()) for c in self.cat_cols}\n",
    "\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.root = self.grow(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        m = len(y)\n",
    "        best = {\"gain\": 0.0}\n",
    "        parent_imp = gini(y)\n",
    "        if m < self.min_samples_split or parent_imp == 0.0:\n",
    "            return None\n",
    "\n",
    "        for col in self.cont_cols:\n",
    "            values = X[col].astype(float).values\n",
    "            uniq = np.unique(values)\n",
    "            if len(uniq) <= 1:\n",
    "                continue\n",
    "            if len(uniq) > self.max_thr_candidates:\n",
    "                qs = np.linspace(0.05, 0.95, self.max_thr_candidates)\n",
    "                thr_cands = np.unique(np.quantile(values, qs))\n",
    "            else:\n",
    "                s = np.sort(uniq)\n",
    "                thr_cands = (s[:-1] + s[1:]) / 2.0\n",
    "\n",
    "            for thr in thr_cands:\n",
    "                left_idx = values <= thr\n",
    "                right_idx = ~left_idx\n",
    "                if left_idx.sum() < self.min_samples_leaf or right_idx.sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                g_left = gini(y[left_idx])\n",
    "                g_right = gini(y[right_idx])\n",
    "                gain = parent_imp - (left_idx.mean()*g_left + right_idx.mean()*g_right)\n",
    "                if gain > best[\"gain\"]:\n",
    "                    best = {\"gain\": gain, \"feature\": col, \"type\": \"cont\", \"thr\": float(thr),\n",
    "                            \"left_idx\": left_idx, \"right_idx\": right_idx}\n",
    "\n",
    "        for col in self.cat_cols:\n",
    "            vals = X[col].astype(str).values\n",
    "            for cat in np.unique(vals):\n",
    "                left_idx = (vals == cat)\n",
    "                right_idx = ~left_idx\n",
    "                if left_idx.sum() < self.min_samples_leaf or right_idx.sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                g_left = gini(y[left_idx])\n",
    "                g_right = gini(y[right_idx])\n",
    "                gain = parent_imp - (left_idx.mean()*g_left + right_idx.mean()*g_right)\n",
    "                if gain > best[\"gain\"]:\n",
    "                    best = {\"gain\": gain, \"feature\": col, \"type\": \"cat\", \"cat\": cat,\n",
    "                            \"left_idx\": left_idx, \"right_idx\": right_idx}\n",
    "\n",
    "        return best if best[\"gain\"] > 0.0 else None\n",
    "\n",
    "    def grow(self, X, y, depth):\n",
    "        if depth >= self.max_depth or len(y) < self.min_samples_split or gini(y) == 0.0:\n",
    "            return make_leaf(y, self.n_classes_)\n",
    "\n",
    "        split = self.best_split(X, y)\n",
    "        if split is None:\n",
    "            return make_leaf(y, self.n_classes_)\n",
    "\n",
    "        if split[\"type\"] == \"cont\":\n",
    "            left = self.grow(X[split[\"left_idx\"]], y[split[\"left_idx\"]], depth+1)\n",
    "            right = self.grow(X[split[\"right_idx\"]], y[split[\"right_idx\"]], depth+1)\n",
    "            return make_cont_node(split[\"feature\"], split[\"thr\"], left, right)\n",
    "        else:\n",
    "            left = self.grow(X[split[\"left_idx\"]], y[split[\"left_idx\"]], depth+1)\n",
    "            right = self.grow(X[split[\"right_idx\"]], y[split[\"right_idx\"]], depth+1)\n",
    "            return make_cat_node(split[\"feature\"], split[\"cat\"], left, right)\n",
    "\n",
    "    def predict_row(self, row, node):\n",
    "        while not node[\"leaf\"]:\n",
    "            val = row[node[\"feature\"]]\n",
    "            if node[\"thr\"] is not None:\n",
    "                go_left = float(val) <= node[\"thr\"]\n",
    "            else:\n",
    "                go_left = str(val) == node[\"cat\"]\n",
    "            node = node[\"left\"] if go_left else node[\"right\"]\n",
    "        return node[\"pred\"], node[\"proba\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        out = []\n",
    "        for _, r in X.iterrows():\n",
    "            p, _ = self.predict_row(r, self.root)\n",
    "            out.append(p)\n",
    "        return np.array(out, dtype=int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        out = []\n",
    "        for _, r in X.iterrows():\n",
    "            _, pr = self.predict_row(r, self.root)\n",
    "            out.append(pr)\n",
    "        return np.array(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Decision Tree Results:\n",
      "Accuracy: 0.9211420802175391\n",
      "F1 Score: 0.9210428000281494\n",
      "Confusion Matrix:\n",
      " [[210  21  14   0   0   0]\n",
      " [ 22 181  11   1   0   0]\n",
      " [ 11   9 177   0   0   0]\n",
      " [  0   0   0 241  16   0]\n",
      " [  0   0   0  11 264   0]\n",
      " [  0   0   0   0   0 282]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       245\n",
      "           1       0.86      0.84      0.85       215\n",
      "           2       0.88      0.90      0.89       197\n",
      "           3       0.95      0.94      0.95       257\n",
      "           4       0.94      0.96      0.95       275\n",
      "           5       1.00      1.00      1.00       282\n",
      "\n",
      "    accuracy                           0.92      1471\n",
      "   macro avg       0.92      0.92      0.92      1471\n",
      "weighted avg       0.92      0.92      0.92      1471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "tree = DecisionTree()\n",
    "tree.fit(\n",
    "    X_tr, y_tr,\n",
    "    cont_cols=X_tr.columns,\n",
    "    cat_cols=[],\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10\n",
    ")\n",
    "\n",
    "y_pred = tree.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "print(\"Custom Decision Tree Results:\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree()\n",
    "tree.fit(\n",
    "    X_train, y_train,\n",
    "    cont_cols=X_train.columns,\n",
    "    cat_cols=[]\n",
    ")\n",
    "y_pred_test = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: (2947,)\n",
      "First 10 test predictions: [5 5 5 5 5 5 5 5 4 5]\n",
      "submissionDT.csv saved\n"
     ]
    }
   ],
   "source": [
    "print(\"Test predictions shape:\", y_pred_test.shape)\n",
    "print(\"First 10 test predictions:\", y_pred_test[:10] + 1)\n",
    "y_pred_test = np.array(y_pred_test, dtype=int)\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": np.arange(1, len(y_pred_test) + 1),\n",
    "    \"Activity\": y_pred_test\n",
    "})\n",
    "assert submission.shape[0] == 2947, \"Row count must be 2947!\"\n",
    "submission.to_csv(\"submissionDT.csv\", index=False)\n",
    "print(\"submissionDT.csv saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN MODELO 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELO 2\n",
    "#K-NEAREST NEIGHBORS\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=5, distance_metric='euclidean', weights='uniform'):\n",
    "        \"\"\"\n",
    "        Inicializa el clasificador KNN.\n",
    "        \n",
    "        Parámetros:\n",
    "        - k: número de vecinos más cercanos a considerar\n",
    "        - distance_metric: métrica de distancia ('euclidean', 'manhattan', 'minkowski', 'cosine')\n",
    "        - weights: tipo de ponderación ('uniform', 'distance')\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weights = weights\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.classes_ = None\n",
    "        self.n_classes_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Almacena los datos de entrenamiento (algoritmo lazy).\n",
    "        \n",
    "        Parámetros:\n",
    "        - X: características de entrenamiento (array-like)\n",
    "        - y: etiquetas de entrenamiento (array-like)\n",
    "        \"\"\"\n",
    "        # Convertir a numpy arrays para mayor flexibilidad\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "        \n",
    "        # Obtener información sobre las clases\n",
    "        self.classes_ = np.unique(self.y_train)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def _calculate_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Calcula la distancia entre dos puntos según la métrica especificada.\n",
    "        \n",
    "        Parámetros:\n",
    "        - x1, x2: vectores de características\n",
    "        \n",
    "        Retorna:\n",
    "        - distancia entre los dos puntos\n",
    "        \"\"\"\n",
    "        x1, x2 = np.array(x1), np.array(x2)\n",
    "        \n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(x1 - x2))\n",
    "        elif self.distance_metric == 'minkowski':\n",
    "            # p=3 para distancia de Minkowski\n",
    "            p = 3\n",
    "            return np.sum(np.abs(x1 - x2) ** p) ** (1/p)\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            # Distancia coseno: 1 - similitud coseno\n",
    "            dot_product = np.dot(x1, x2)\n",
    "            norm_x1 = np.linalg.norm(x1)\n",
    "            norm_x2 = np.linalg.norm(x2)\n",
    "            if norm_x1 == 0 or norm_x2 == 0:\n",
    "                return 1.0  # Máxima distancia si algún vector es cero\n",
    "            cosine_sim = dot_product / (norm_x1 * norm_x2)\n",
    "            return 1 - cosine_sim\n",
    "        else:\n",
    "            raise ValueError(f\"Métrica de distancia '{self.distance_metric}' no soportada. \"\n",
    "                           \"Opciones disponibles: 'euclidean', 'manhattan', 'minkowski', 'cosine'\")\n",
    "    \n",
    "    def _get_neighbors(self, x):\n",
    "        \"\"\"\n",
    "        Encuentra los k vecinos más cercanos para un punto dado.\n",
    "        \n",
    "        Parámetros:\n",
    "        - x: punto de consulta\n",
    "        \n",
    "        Retorna:\n",
    "        - lista de tuplas (distancia, etiqueta) de los k vecinos más cercanos\n",
    "        \"\"\"\n",
    "        distances = []\n",
    "        for i, train_sample in enumerate(self.X_train):\n",
    "            dist = self._calculate_distance(x, train_sample)\n",
    "            distances.append((dist, self.y_train[i]))\n",
    "        \n",
    "        # Ordenar por distancia y tomar los k más cercanos\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        k_actual = min(self.k, len(distances))  # En caso de que k > número de muestras\n",
    "        neighbors = distances[:k_actual]\n",
    "        return neighbors\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice las clases para múltiples muestras.\n",
    "        \n",
    "        Parámetros:\n",
    "        - X: matriz de características para predicción\n",
    "        \n",
    "        Retorna:\n",
    "        - array de predicciones\n",
    "        \"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"El modelo debe ser entrenado antes de hacer predicciones\")\n",
    "        \n",
    "        X = np.array(X)\n",
    "        # Manejar tanto vectores individuales como matrices\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            neighbors = self._get_neighbors(x)\n",
    "            \n",
    "            if self.weights == 'uniform':\n",
    "                # Votación simple\n",
    "                neighbor_classes = [neighbor[1] for neighbor in neighbors]\n",
    "                most_common = Counter(neighbor_classes).most_common(1)[0][0]\n",
    "                predictions.append(most_common)\n",
    "                \n",
    "            elif self.weights == 'distance':\n",
    "                # Votación ponderada por distancia inversa\n",
    "                class_weights = {}\n",
    "                for dist, cls in neighbors:\n",
    "                    weight = 1 / (dist + 1e-8)  # Evitar división por cero\n",
    "                    class_weights[cls] = class_weights.get(cls, 0) + weight\n",
    "                \n",
    "                # Seleccionar la clase con mayor peso\n",
    "                predicted_class = max(class_weights, key=class_weights.get)\n",
    "                predictions.append(predicted_class)\n",
    "            else:\n",
    "                raise ValueError(f\"Tipo de ponderación '{self.weights}' no soportado. \"\n",
    "                               \"Opciones disponibles: 'uniform', 'distance'\")\n",
    "                \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Retorna probabilidades de pertenencia a cada clase.\n",
    "        \n",
    "        Parámetros:\n",
    "        - X: matriz de características para predicción\n",
    "        \n",
    "        Retorna:\n",
    "        - matriz de probabilidades (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"El modelo debe ser entrenado antes de hacer predicciones\")\n",
    "        \n",
    "        X = np.array(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        probas = []\n",
    "        for x in X:\n",
    "            neighbors = self._get_neighbors(x)\n",
    "            neighbor_classes = [neighbor[1] for neighbor in neighbors]\n",
    "            \n",
    "            # Inicializar probabilidades para todas las clases\n",
    "            class_proba = {cls: 0.0 for cls in self.classes_}\n",
    "            \n",
    "            if self.weights == 'uniform':\n",
    "                counts = Counter(neighbor_classes)\n",
    "                total = len(neighbor_classes)\n",
    "                for cls in self.classes_:\n",
    "                    class_proba[cls] = counts.get(cls, 0) / total\n",
    "                \n",
    "            elif self.weights == 'distance':\n",
    "                class_weights = {cls: 0.0 for cls in self.classes_}\n",
    "                total_weight = 0\n",
    "                for dist, cls in neighbors:\n",
    "                    weight = 1 / (dist + 1e-8)\n",
    "                    class_weights[cls] += weight\n",
    "                    total_weight += weight\n",
    "                \n",
    "                for cls in self.classes_:\n",
    "                    class_proba[cls] = class_weights[cls] / total_weight if total_weight > 0 else 0\n",
    "            \n",
    "            # Convertir a array en el orden de self.classes_\n",
    "            proba_array = [class_proba[cls] for cls in self.classes_]\n",
    "            probas.append(proba_array)\n",
    "            \n",
    "        return np.array(probas)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcula la precisión del modelo en el conjunto de datos dado.\n",
    "        \n",
    "        Parámetros:\n",
    "        - X: características de prueba\n",
    "        - y: etiquetas verdaderas\n",
    "        \n",
    "        Retorna:\n",
    "        - precisión (accuracy)\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tokio\\miniconda3\\envs\\faceenv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_rp_train, y_rp_train, test_size=0.2, random_state=42, stratify=y_rp_train\n",
    ")\n",
    "KNN_model = KNN(k=5, distance_metric='euclidean', weights='uniform')\n",
    "KNN_model.fit(X_tr, y_tr)\n",
    "y_pred = KNN_model.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "print(\"\\n=== KNN from Scratch Results ===\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\",report)"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "720790fa37304849b6c3457771488ff4",
  "kernelspec": {
   "display_name": "faceenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
